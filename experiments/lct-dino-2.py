import torch, torchvision, torchaudio
print("torch:", torch.__version__)
print("torchvision:", torchvision.__version__)
print("torchaudio:", torchaudio.__version__)
print("cuda:", torch.version.cuda)
import numpy, scipy, sklearn
print("numpy:", numpy.__version__)
print("scipy:", scipy.__version__)
print("sklearn:", sklearn.__version__)


import os, glob, json, random, math
import numpy as np
import contextlib
from PIL import Image, ImageDraw

BASE_DIR = "/kaggle/working"

IMG_DIR      = "/kaggle/input/lct-low/LCT/Ð“Ñ€ÑƒÐ¿Ð¿Ð¾Ð²Ñ‹Ðµ Ð´Ð»Ñ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸"  # Ð³Ñ€ÑƒÐ¿Ð¿Ð¾Ð²Ñ‹Ðµ Ñ„Ð¾Ñ‚Ð¾
CROPS_TRAIN  = "/kaggle/input/lct-low/LCT"                           # ÐºÑ€Ð¾Ð¿Ñ‹ Ð´Ð»Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ (Ð¿Ð¾Ð´Ð¿Ð°Ð¿ÐºÐ¸-ÐºÐ»Ð°ÑÑÑ‹)
CROPS_REF    = "/kaggle/input/lct-ref/REF"                           # ÑÑ‚Ð°Ð»Ð¾Ð½Ñ‹ (8â€“16 Ð½Ð° ÐºÐ»Ð°ÑÑ)

OUT_VLM      = os.path.join(BASE_DIR, "out_vlm")      # Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸ Ð¾Ñ‚ GroundingDINO
OUT_METRIC   = os.path.join(BASE_DIR, "out_metric")   # Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð²Ð¸Ð·ÑƒÐ°Ð»ÐºÐ¸ Ð¸ JSON
BEST_PATH    = os.path.join(BASE_DIR, "embedder_arcface_best.pth")
PROTO_PATH   = os.path.join(BASE_DIR, "prototypes.npz")
PRED_VLM     = os.path.join(OUT_VLM, "predictions_vlm.json")
PRED_FINAL   = os.path.join(OUT_METRIC, "predictions_final.json")     # Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð²Ð¸Ð·ÑƒÐ°Ð»ÐºÐ¸ Ð¸ JSON

for d in [IMG_DIR, CROPS_TRAIN, CROPS_REF, OUT_VLM, OUT_METRIC]:
    os.makedirs(d, exist_ok=True)

# Ð“Ð¸Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
IMG_SIZE = 224
BATCH    = 64
EPOCHS   = 15
LR       = 1e-3
SEED     = 42
NUM_WORKERS = 2
# ÐŸÐ¾Ñ€Ð¾Ð³Ð¾Ð²Ñ‹Ðµ Ð¿Ð°Ñ€-Ñ€Ñ‹ Ð´Ð»Ñ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸
ALPHA        = 0.5   # ÑÐ¼ÐµÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ centroid Ð¸ kNN
TOPK         = 5
THR_UNKNOWN  = 0.10  # Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ cos Ð´Ð¾ Ð¿Ñ€Ð¾Ñ‚Ð¾Ñ‚Ð¸Ð¿Ð°
THR_MARGIN   = 0.03  # Ð¾Ñ‚Ñ€Ñ‹Ð² top1-top2
IOU_THR      = 0.55  # Ð´Ð»Ñ class-wise NMS

## ÐžÐ±ÑƒÑ‡Ð¸Ð¼ ÑÐ¼Ð±ÐµÐ´Ð´ÐµÑ€: DinoV2 + ArcFace (Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ)
# ÐœÐ¾Ð´ÐµÐ»ÑŒ
MODEL_NAME = "vit_base_patch14_dinov2.lvd142m"  # Ð¸Ð»Ð¸ "vit_base_patch14_dinov2.lvd142m"
EMB_DIM    = 128
FREEZE_BACKBONE = True      # 1-Ð¹ ÑÑ‚Ð°Ð¿: ÑƒÑ‡Ð¸Ð¼ Ð³Ð¾Ð»Ð¾Ð²Ñƒ
# ==== IMPORTS & DEVICE ====
import glob, json, random, math
import numpy as np
from PIL import Image, ImageDraw
import unicodedata as ud

import torch, torch.nn as nn, torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from tqdm.notebook import tqdm
import timm

device = "cuda" if torch.cuda.is_available() else "cpu"
print("device:", device)

def set_seed(seed=42):
    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
set_seed(SEED)

try:
    torch.set_float32_matmul_precision("high")
except Exception:
    pass

# ==== DATASET & UTILS ====

def _norm(s: str) -> str:
    return ud.normalize("NFKC", s).strip().lower()

def list_items(root):
    assert os.path.isdir(root), f"ÐŸÐ°Ð¿ÐºÐ° Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°: {root}"
    SKIP_SUBSTR = [_norm("Ð³Ñ€ÑƒÐ¿Ð¿Ð¾Ð²Ñ‹Ðµ"), _norm("Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ðº"), _norm("Ð»Ð¸Ð½ÐµÐ¹Ðº")]

    items, classes = [], []
    for c in sorted(os.listdir(root)):
        if c.startswith("."): 
            continue
        pth = os.path.join(root, c)
        if not os.path.isdir(pth):
            continue
        if any(s in _norm(c) for s in SKIP_SUBSTR):
            continue
        classes.append(c)
        for f in glob.glob(os.path.join(pth, "*")):
            if f.lower().endswith((".jpg",".jpeg",".png",".bmp",".webp")):
                items.append((f, c))

    assert len(classes) > 0, "ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð½Ð¸ Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÐºÐ»Ð°ÑÑÐ° Ð² CROPS_TRAIN"
    cls2id = {c:i for i,c in enumerate(classes)}
    return items, cls2id

class ImgDS(Dataset):
    def __init__(self, items, cls2id, tfm):
        self.items, self.cls2id, self.tfm = items, cls2id, tfm
    def __len__(self): return len(self.items)
    def __getitem__(self, i):
        p,c = self.items[i]
        with Image.open(p).convert("RGB") as im:
            x = self.tfm(im)
        return x, self.cls2id[c]
# ==== TRANSFORMS (timm) ====
_tmp = timm.create_model(MODEL_NAME, pretrained=True, num_classes=0, global_pool="token")
data_cfg = timm.data.resolve_model_data_config(_tmp)
if IMG_SIZE is not None:
    data_cfg["input_size"] = (3, IMG_SIZE, IMG_SIZE)

tfm_train = timm.data.create_transform(**data_cfg, is_training=True)
tfm_val   = timm.data.create_transform(**data_cfg, is_training=False)

# ==== MODEL & ARC ====

class EmbedNet(nn.Module):
    def __init__(self, dim=128, model_name=MODEL_NAME, pool="token", freeze_backbone=False, img_size=IMG_SIZE):
        super().__init__()
        self.backbone = timm.create_model(
            model_name,
            pretrained=True,
            num_classes=0,
            global_pool=pool,
            img_size=img_size  # ÑÐ¾Ð³Ð»Ð°ÑÑƒÐµÐ¼ Ñ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð°Ð¼Ð¸
        )
        # Ð Ð°Ð·Ñ€ÐµÑˆÐ°ÐµÐ¼ Ð²Ñ…Ð¾Ð´ Ð½Ðµ Ñ€Ð¾Ð²Ð½Ð¾ "Ñ€Ð¾Ð´Ð½Ð¾Ð³Ð¾" Ñ€Ð°Ð·Ð¼ÐµÑ€Ð° (Ð±ÐµÐ· Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ Ð¿ÐµÑ€ÐµÑÐµÐ¼Ð¿Ð»Ð¾Ð²ÐºÐ¸ pos-ÑÐ¼Ð±ÐµÐ´Ð¾Ð²)
        if hasattr(self.backbone, "patch_embed"):
            try:
                self.backbone.patch_embed.strict_img_size = False
            except Exception:
                pass
        # Ð’ÐÐ–ÐÐž: ÐÐ• Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ dynamic_img_size â€” ÑÑ‚Ð¾ Ð¸ Ð»Ð¾Ð¼Ð°Ð»Ð¾ Ñ„Ð¾Ñ€Ð¼Ñƒ
        # if hasattr(self.backbone, "dynamic_img_size"):
        #     self.backbone.dynamic_img_size = True  # Ð£Ð”ÐÐ›Ð•ÐÐž

        feat_dim = self.backbone.num_features
        if freeze_backbone:
            for p in self.backbone.parameters():
                p.requires_grad = False
        self.head = nn.Sequential(
            nn.Linear(feat_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Linear(512, dim),
        )

    def forward(self, x):
        f = self.backbone(x)                 # [B, feat_dim]
        z = F.normalize(self.head(f), dim=1) # [B, dim]
        return z

class ArcMarginProduct(nn.Module):
    def __init__(self, in_dim, n_classes, s=30.0, m=0.30):
        super().__init__()
        self.W = nn.Parameter(torch.randn(n_classes, in_dim))
        nn.init.xavier_uniform_(self.W)
        self.s, self.m = s, m
    def forward(self, z, y):
        W = F.normalize(self.W, dim=1)
        cos = F.linear(z, W)
        one_hot = F.one_hot(y, num_classes=W.size(0)).float().to(z.device)
        return self.s * (cos - one_hot*self.m)

# Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
model = EmbedNet(dim=EMB_DIM, freeze_backbone=FREEZE_BACKBONE, img_size=IMG_SIZE).to(device)
arc   = ArcMarginProduct(EMB_DIM, n_classes=1).to(device)  # Ð¿ÐµÑ€ÐµÐ¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ð¼ n_classes Ð½Ð¸Ð¶Ðµ
ce    = nn.CrossEntropyLoss()


# --- Albumentations Ð°ÑƒÐ³Ð¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ ÑÐ¼Ð±ÐµÐ´Ð´ÐµÑ€Ð° ---
# pip: !pip -q install albumentations>=1.4.0  # (Ð½Ð° Kaggle Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ ÑƒÐ¶Ðµ ÐµÑÑ‚ÑŒ)
import albumentations as A
from albumentations.pytorch import ToTensorV2

# mean/std Ð±ÐµÑ€Ñ‘Ð¼ Ð¸Ð· timm data_cfg, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð²ÑÑ‘ ÑÐ¾Ð²Ð¿Ð°Ð´Ð°Ð»Ð¾ Ñ Ð±ÑÐºÐ±Ð¾Ð½Ð¾Ð¼
_MEAN = data_cfg.get('mean', (0.5, 0.5, 0.5))
_STD  = data_cfg.get('std',  (0.5, 0.5, 0.5))

IMG_PAD_VALUE = (114, 114, 114)  # Ñ„Ð¾Ð½ ÑÑ‚Ð¾Ð»Ð°/Ð±ÑƒÐ¼Ð°Ð³Ð¸

train_aug = A.Compose([
    A.LongestMaxSize(max_size=IMG_SIZE),
    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=0, value=IMG_PAD_VALUE),
    A.ShiftScaleRotate(shift_limit=0.04, scale_limit=0.15, rotate_limit=35,
                       border_mode=0, value=IMG_PAD_VALUE, p=0.8),
    A.HorizontalFlip(p=0.5),
    A.Perspective(scale=(0.02, 0.06), p=0.2),
    A.ColorJitter(0.3, 0.3, 0.3, 0.05, p=0.7),
    A.RandomGamma(gamma_limit=(80, 120), p=0.3),
    A.CLAHE(clip_limit=(1, 3), p=0.1),
    A.GaussNoise(var_limit=(5.0, 25.0), p=0.2),
    A.MotionBlur(blur_limit=3, p=0.1),
    A.ImageCompression(quality_lower=60, quality_upper=95, p=0.2),
    A.CoarseDropout(max_holes=4,
                    max_height=int(0.25*IMG_SIZE), max_width=int(0.25*IMG_SIZE),
                    min_holes=1, min_height=16, min_width=16,
                    fill_value=IMG_PAD_VALUE, p=0.5),
    A.Normalize(mean=_MEAN, std=_STD),
    ToTensorV2(),
])

val_aug = A.Compose([
    A.LongestMaxSize(max_size=IMG_SIZE),
    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=0, value=IMG_PAD_VALUE),
    A.Normalize(mean=_MEAN, std=_STD),
    ToTensorV2(),
])

# --- Dataset, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ Albumentations ---
class AlbDS(torch.utils.data.Dataset):
    def __init__(self, items, cls2id, aug):
        self.items = items
        self.cls2id = cls2id
        self.aug = aug

    def __len__(self): return len(self.items)

    def __getitem__(self, i):
        p, c = self.items[i]
        # PIL -> numpy(BGR/RGB Ð½ÐµÐ²Ð°Ð¶Ð½Ð¾ Ð´Ð»Ñ A, Ð½Ð¾ Ð¼Ñ‹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ RGB)
        img = np.array(Image.open(p).convert("RGB"))
        out = self.aug(image=img)
        x = out["image"]  # torch.Tensor [C,H,W], ÑƒÐ¶Ðµ Ð½Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½
        y = self.cls2id[c]
        return x, y

# Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ð¹ Ð¾Ð±Ñ‘Ñ€Ñ‚Ñ‡Ð¸Ðº, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ñ‚Ðµ Ð¶Ðµ ÑÐ°Ð¼Ñ‹Ðµ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ
# Ð´Ð»Ñ Ð¿Ñ€Ð¾Ñ‚Ð¾Ñ‚Ð¸Ð¿Ð¾Ð² Ð¸ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ° ÐºÑ€Ð¾Ð¿Ð¾Ð²: Ð²Ñ‹Ð·Ð¾Ð² ÐºÐ°Ðº tfm(image_PIL) -> torch.Tensor
class AlbWrap:
    def __init__(self, aug): self.aug = aug
    def __call__(self, pil_img):
        arr = np.array(pil_img.convert("RGB"))
        return self.aug(image=arr)["image"]

# ==== DATA & TRAIN ====
items, cls2id = list_items(CROPS_TRAIN)
train_items, val_items = train_test_split(items, test_size=0.2,
                                          stratify=[c for _,c in items], random_state=SEED)

# Ð¿ÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ arc Ñ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¼ Ñ‡Ð¸ÑÐ»Ð¾Ð¼ ÐºÐ»Ð°ÑÑÐ¾Ð²
del arc
arc = ArcMarginProduct(EMB_DIM, n_classes=len(cls2id)).to(device)

pin = torch.cuda.is_available()
dl_train = DataLoader(AlbDS(train_items, cls2id, train_aug), batch_size=BATCH, shuffle=True,
                      num_workers=NUM_WORKERS, pin_memory=pin, persistent_workers=pin and NUM_WORKERS>0)
dl_val   = DataLoader(AlbDS(val_items,   cls2id, val_aug),   batch_size=BATCH, shuffle=False,
                      num_workers=NUM_WORKERS, pin_memory=pin, persistent_workers=pin and NUM_WORKERS>0)

# Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€
if FREEZE_BACKBONE:
    params = list(model.head.parameters()) + list(arc.parameters())
    opt = torch.optim.AdamW(params, lr=LR, weight_decay=1e-4)
else:
    opt = torch.optim.AdamW([
        {"params": model.backbone.parameters(), "lr": max(LR*0.2, 5e-5), "weight_decay": 0.05},
        {"params": model.head.parameters(),     "lr": LR,                "weight_decay": 1e-4},
        {"params": arc.parameters(),            "lr": LR,                "weight_decay": 1e-4},
    ])

scaler = torch.amp.GradScaler(device='cuda', enabled=torch.cuda.is_available())
autocast_ctx = (torch.amp.autocast(device_type='cuda', dtype=torch.float16, enabled=torch.cuda.is_available())
                if torch.cuda.is_available()
                else contextlib.nullcontext())
best_acc = 0.0
os.makedirs(os.path.dirname(BEST_PATH), exist_ok=True)

for ep in range(1, EPOCHS+1):
    model.train(); arc.train()
    tot=0; cnt=0
    pbar = tqdm(dl_train, desc=f"Epoch {ep}/{EPOCHS}", leave=False)
    for x,y in pbar:
        x,y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)
        with autocast_ctx:
            z = model(x)
            logits = arc(z, y)
            loss = ce(logits, y)
        opt.zero_grad(set_to_none=True)
        scaler.scale(loss).backward()
        scaler.step(opt)
        scaler.update()
        tot += loss.item()*x.size(0); cnt += x.size(0)
        pbar.set_postfix({"loss": f"{loss.item():.4f}"})

    model.eval(); arc.eval()
    correct=0; total=0
    with torch.no_grad():
        for x,y in dl_val:
            x,y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)
            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
                z = model(x); logits = arc(z,y)
            pred = logits.argmax(1)
            correct += (pred==y).sum().item(); total += y.size(0)

    val_acc = correct / max(1,total)
    print(f"Epoch {ep:02d} | train_loss={tot/max(1,cnt):.4f} | val_acc={val_acc:.3f}")

    if val_acc > best_acc:
        best_acc = val_acc
        torch.save({
            "model": model.state_dict(),
            "cls2id": cls2id,
            "epoch": ep,
            "val_acc": val_acc,
            "model_name": MODEL_NAME,
            "emb_dim": EMB_DIM,
            "freeze_backbone": FREEZE_BACKBONE,
            "data_cfg": data_cfg,        # <-- ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð¾Ð²!
            "img_size": IMG_SIZE         # <-- Ð¸ Ñ€Ð°Ð·Ð¼ÐµÑ€
        }, BEST_PATH)
        print(f"ðŸŒŸ ÐÐ¾Ð²Ñ‹Ð¹ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ñ‡ÐµÐºÐ¿Ð¾Ð¸Ð½Ñ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½Ñ‘Ð½: {BEST_PATH} (acc={val_acc:.3f})")

print("âœ… ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾. Ð›ÑƒÑ‡ÑˆÐ¸Ð¹ acc =", best_acc)

# Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ñ‹ Ð´Ð»Ñ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ°/Ð¿Ñ€Ð¾Ñ‚Ð¾Ñ‚Ð¸Ð¿Ð¾Ð²
tfm_proto = AlbWrap(val_aug)   # Ñ‚Ðµ Ð¶Ðµ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ/Ð¿Ð°Ð´Ð´Ð¸Ð½Ð³/resize
tfm_infer = tfm_proto

import os, torch
ckpt_path = "/kaggle/working/embedder_arcface_best.pth"
print("Ð¤Ð°Ð¹Ð» ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚:", os.path.exists(ckpt_path), "| Ñ€Ð°Ð·Ð¼ÐµÑ€, Ð±Ð°Ð¹Ñ‚:", os.path.getsize(ckpt_path) if os.path.exists(ckpt_path) else 0)

if os.path.exists(ckpt_path):
    ckpt = torch.load(ckpt_path, map_location="cpu")
    print("ÐšÐ»ÑŽÑ‡Ð¸ Ð² Ñ‡ÐµÐºÐ¿Ð¾Ð¹Ð½Ñ‚Ðµ:", list(ckpt.keys()))
    print("Ð§Ð¸ÑÐ»Ð¾ ÐºÐ»Ð°ÑÑÐ¾Ð²:", len(ckpt["cls2id"]))

for cls, idx in ckpt["cls2id"].items():
    print(f"{idx:2d} â†’ {cls}")

#  0 â†’ 1 ÐžÑ‚Ð²ÐµÑ€Ñ‚ÐºÐ° Â«-Â»
#  1 â†’ 10 ÐšÐ»ÑŽÑ‡ Ñ€Ð¾Ð¶ÐºÐ¾Ð²Ñ‹Ð¸Ì†Ð½Ð°ÐºÐ¸Ð´Ð½Ð¾Ð¸Ì†  Â¾
#  2 â†’ 11 Ð‘Ð¾ÐºÐ¾Ñ€ÐµÐ·Ñ‹
#  3 â†’ 2 ÐžÑ‚Ð²ÐµÑ€Ñ‚ÐºÐ° Â«+Â»
#  4 â†’ 3 ÐžÑ‚Ð²ÐµÑ€Ñ‚ÐºÐ° Ð½Ð° ÑÐ¼ÐµÑ‰ÐµÐ½Ð½Ñ‹Ð¸Ì† ÐºÑ€ÐµÑÑ‚
#  5 â†’ 4 ÐšÐ¾Ð»Ð¾Ð²Ð¾Ñ€Ð¾Ñ‚
#  6 â†’ 5 ÐŸÐ°ÑÑÐ°Ñ‚Ð¸Ð¶Ð¸ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ñ‹Ðµ
#  7 â†’ 6 ÐŸÐ°ÑÑÐ°Ñ‚Ð¸Ð¶Ð¸
#  8 â†’ 7 Ð¨ÑÑ€Ð½Ð¸Ñ†Ð°
#  9 â†’ 8 Ð Ð°Ð·Ð²Ð¾Ð´Ð½Ð¾Ð¸Ì† ÐºÐ»ÑŽÑ‡
# 10 â†’ 9 ÐžÑ‚ÐºÑ€Ñ‹Ð²Ð°ÑˆÐºÐ° Ð´Ð»Ñ Ð±Ð°Ð½Ð¾Ðº Ñ Ð¼Ð°ÑÐ»Ð¾Ð¼

## ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ð¼ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð½Ð° Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°Ñ… ÐºÐ»Ð°ÑÑÐ¾Ð² (Ñ†ÐµÐ½Ñ‚Ñ€Ð¾Ð¸Ð´Ñ‹ + ÑÑ‚ÐµÐºÐ¸ ÑÑ‚Ð°Ð»Ð¾Ð½Ð¾Ð²)
# ==== PROTOTYPES ====
import numpy as np

ckpt = torch.load(BEST_PATH, map_location="cpu")
cls2id = ckpt["cls2id"]; id2cls = {v:k for k,v in cls2id.items()}

# ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ð° CPU Ñ Ñ‚ÐµÐ¼ Ð¶Ðµ img_size, Ñ‡Ñ‚Ð¾ Ð² ckpt
proto_model = EmbedNet(EMB_DIM, model_name=ckpt.get("model_name", MODEL_NAME),
                       freeze_backbone=True, img_size=ckpt.get("img_size", IMG_SIZE))
proto_model.load_state_dict(ckpt["model"])
proto_model.eval()

centroids, stacks = {}, {}
with torch.no_grad():
    for c in sorted(os.listdir(CROPS_REF)):
        pth = os.path.join(CROPS_REF, c)
        if not os.path.isdir(pth): 
            continue
        vecs=[]
        for f in glob.glob(os.path.join(pth, "*")):
            if not f.lower().endswith((".jpg",".png",".jpeg",".bmp",".webp")):
                continue
            with Image.open(f).convert("RGB") as im:
                x = tfm_proto(im).unsqueeze(0)  # CPU
            z = proto_model(x).squeeze(0).numpy()
            z /= max(np.linalg.norm(z), 1e-12)
            vecs.append(z)
        if vecs:
            M = np.stack(vecs, axis=0).astype(np.float32)
            mu = M.mean(0); mu /= max(np.linalg.norm(mu), 1e-12)
            centroids[c] = mu
            stacks[c]    = M

np.savez_compressed(PROTO_PATH,
                    centroids=centroids, stacks=stacks, id2cls=id2cls, cls2id=cls2id)
print("âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸", PROTO_PATH)


NPZ = "/kaggle/working/prototypes.npz"
npz = np.load(NPZ, allow_pickle=True)
centroids = npz["centroids"].item()
stacks    = npz["stacks"].item()
print("\nÐšÐ»Ð°ÑÑÐ¾Ð² Ð² prototypes.npz:", len(centroids))

# ÐšÐ»Ð°ÑÑÐ¾Ð² Ð² prototypes.npz: 11

## GroundingDino: Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ñ Ð±Ð¾ÐºÑÐ¾Ð² Ð¿Ð¾ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð°Ð¼ (Ð±ÐµÐ· Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ)

# ==== GroundingDINO: Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ñ Ð¿Ð¾ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¼ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑÐ¼ ====
import transformers as hf
from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection
from torchvision.ops import nms

print("transformers:", hf.__version__)
print("torch:", torch.__version__)
use_cuda = torch.cuda.is_available()
gd_device = "cuda" if use_cuda else "cpu"

# ÐšÐ»Ð°ÑÑ-Ð»Ð¸ÑÑ‚
class_list = [
    "tool"
]
idx2class  = {i:c for i,c in enumerate(class_list)}
text_prompt = " . ".join(class_list) + " ."

MODEL_NAME_DINO = "IDEA-Research/grounding-dino-base"  # tiny Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ; base Ñ‚Ð¾Ñ‡Ð½ÐµÐµ
processor  = AutoProcessor.from_pretrained(MODEL_NAME_DINO)
gdinomodel = AutoModelForZeroShotObjectDetection.from_pretrained(MODEL_NAME_DINO).to(gd_device).eval()

BOX_THR, TEXT_THR = 0.25, 0.25
VLM_IOU_THR, TOPK_PER_CLS = 0.55, 50
DO_VIS, BATCH_SIZE, SAVE_EVERY, RESUME = False, 2, 50, False

all_imgs = sorted([p for p in glob.glob(os.path.join(IMG_DIR, "*"))
                   if p.lower().endswith((".jpg",".jpeg",".png",".bmp",".webp"))])
print(f"ðŸ–¼ï¸ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ð¹: {len(all_imgs)}")

vlm_results = []
done_set = set()
pred_path = PRED_VLM  # <-- Ð‘Ð«Ð›Ðž ÐÐ•ÐžÐŸÐ Ð•Ð”Ð•Ð›Ð•ÐÐž, Ñ‚ÐµÐ¿ÐµÑ€ÑŒ ÐµÑÑ‚ÑŒ

def save_json_atomic(path, data):
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)

def to_tensor_safe(x, dtype=None, dev="cpu"):
    if isinstance(x, torch.Tensor):
        return x.to(device=dev, dtype=dtype) if (dtype or x.device.type != dev) else x
    return torch.tensor(x, dtype=dtype, device=dev)

def extract_label_names(res, idx2class):
    for k in ("text_labels","texts","phrases"):
        if k in res and res[k] is not None:
            try:
                return [str(s) for s in (res[k].tolist() if isinstance(res[k], torch.Tensor) else list(res[k]))]
            except Exception:
                pass
    for k in ("labels","label_ids","label_indices"):
        if k in res and res[k] is not None:
            raw = res[k]
            if isinstance(raw, torch.Tensor):
                idxs = raw.detach().cpu().to(torch.int64).tolist()
            else:
                seq = list(raw)
                if not seq: return []
                if isinstance(seq[0], str): return [str(s) for s in seq]
                try: idxs = [int(i) for i in seq]
                except Exception: return [str(x) for x in seq]
            return [idx2class[i] if 0 <= int(i) < len(idx2class) else None for i in idxs]
    return []

def run_batch(paths):
    imgs = [Image.open(p).convert("RGB") for p in paths]
    inputs = processor(images=imgs, text=[text_prompt]*len(imgs), return_tensors="pt").to(gd_device)
    with torch.no_grad():
        if use_cuda:
            with torch.autocast(device_type="cuda", dtype=torch.float16):
                outputs = gdinomodel(**inputs)
        else:
            outputs = gdinomodel(**inputs)

    results = processor.post_process_grounded_object_detection(
        outputs=outputs, input_ids=inputs.input_ids,
        box_threshold=BOX_THR, text_threshold=TEXT_THR,
        target_sizes=[im.size[::-1] for im in imgs],
    )

    batch_dets=[]
    for im,res in zip(imgs, results):
        boxes  = to_tensor_safe(res.get("boxes", []),  dtype=torch.float32, dev="cpu")
        scores = to_tensor_safe(res.get("scores", []), dtype=torch.float32, dev="cpu")
        labels_iter = extract_label_names(res, idx2class)

        n = min(len(boxes), len(scores), len(labels_iter))
        if n == 0:
            batch_dets.append([])
            continue
        boxes, scores, labels_iter = boxes[:n], scores[:n], labels_iter[:n]

        dets_by_cls = {}
        for bb, sc, name in zip(boxes, scores, labels_iter):
            if not name: continue
            dets_by_cls.setdefault(name, []).append((bb, float(sc)))

        dets = []
        for cls_name, arr in dets_by_cls.items():
            b = torch.stack([a[0] if isinstance(a[0], torch.Tensor) else torch.tensor(a[0], dtype=torch.float32) for a in arr], dim=0)
            s = torch.tensor([a[1] for a in arr], dtype=torch.float32)
            if use_cuda: b,s = b.to("cuda"), s.to("cuda")
            if len(s) > TOPK_PER_CLS:
                topk = torch.topk(s, TOPK_PER_CLS).indices
                b, s = b[topk], s[topk]
            if b.numel()==0: continue
            keep = nms(b, s, VLM_IOU_THR)
            b, s = b[keep].to("cpu"), s[keep].to("cpu")
            for bb, sc in zip(b, s):
                x1,y1,x2,y2 = [float(v) for v in bb.tolist()]
                dets.append({"class": cls_name, "score_vlm": float(sc), "bbox_xyxy": [x1,y1,x2,y2]})
        batch_dets.append(dets)

    for im in imgs:
        try: im.close()
        except Exception: pass

    return batch_dets

buf=[]
for i in tqdm(range(0, len(all_imgs), BATCH_SIZE), desc="GroundingDINO"):
    chunk = all_imgs[i:i+BATCH_SIZE]
    try:
        dets_list = run_batch(chunk)
    except Exception as e:
        print(f"âš ï¸ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð² run_batch [{i}:{i+BATCH_SIZE}]: {e}")
        continue

    if DO_VIS:
        for p, dets in zip(chunk, dets_list):
            try:
                with Image.open(p).convert("RGB") as im:
                    d = ImageDraw.Draw(im)
                    for det in dets:
                        x1,y1,x2,y2 = det["bbox_xyxy"]
                        d.rectangle([x1,y1,x2,y2], outline=(0,255,0), width=3)
                        d.text((max(0,x1+3), max(0,y1-14)), f'{det["class"]}:{det["score_vlm"]:.2f}', fill=(0,255,0))
                    im.save(os.path.join(OUT_VLM, os.path.splitext(os.path.basename(p))[0] + "_vlm.jpg"))
            except Exception as e:
                print("âš ï¸ Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ:", e)

    for p, dets in zip(chunk, dets_list):
        rec = {"image": os.path.basename(p), "detections": dets}
        vlm_results.append(rec); buf.append(rec)

    if len(buf) >= SAVE_EVERY:
        save_json_atomic(pred_path, vlm_results)
        buf.clear()

if buf or not os.path.exists(pred_path):
    save_json_atomic(pred_path, vlm_results)

print("âœ… GroundingDINO Ð³Ð¾Ñ‚Ð¾Ð²Ð¾:", pred_path)


#  GroundingDINO Ð³Ð¾Ñ‚Ð¾Ð²Ð¾: /kaggle/working/out_vlm/predictions_vlm.json
## ÐœÐµÑ‚Ñ€Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¿ÐµÑ€ÐµÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ + class-wise NMS
from torchvision.ops import nms
from torchvision import transforms
# Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÑÐ¼Ð±ÐµÐ´Ð´ÐµÑ€Ð° Ð¸ Ð¿Ñ€Ð¾Ñ‚Ð¾Ñ‚Ð¸Ð¿Ð¾Ð²
ckpt = torch.load(os.path.join(BASE_DIR,"embedder_arcface_best.pth"), map_location=device)
clf_model = EmbedNet(128).to(device)
clf_model.load_state_dict(ckpt["model"])
clf_model.eval()

npz = np.load(os.path.join(BASE_DIR,"prototypes.npz"), allow_pickle=True)
# Ð°ÑÑÐµÑ€Ñ‚ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ñ‚Ð¾Ñ‚Ð¸Ð¿Ð¾Ð²
assert len(centroids) > 0, "centroids Ð¿ÑƒÑÑ‚Ð¾Ð¹ â€” Ð¿ÐµÑ€ÐµÑÐ¾Ð·Ð´Ð°Ð¹ prototypes.npz Ð¸Ð· CROPS_REF"
for c, M in stacks.items():
    if M is None or len(M) == 0:
        print(f"âš ï¸ Ð’ ÐºÐ»Ð°ÑÑÐµ '{c}' Ð½ÐµÑ‚ ÑÑ‚Ð°Ð»Ð¾Ð½Ð¾Ð² Ð² stacks â€” Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰Ñƒ ÐµÐ³Ð¾")
# Ð¾Ñ‚Ð±Ñ€Ð¾ÑÐ¸Ð¼ ÐºÐ»Ð°ÑÑÑ‹ Ð±ÐµÐ· ÑÑ‚ÐµÐºÐ¾Ð²
centroids = {c:v for c,v in centroids.items() if c in stacks and len(stacks[c])>0}
stacks    = {c:stacks[c] for c in centroids.keys()}
assert len(centroids) > 0, "ÐŸÐ¾ÑÐ»Ðµ Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ð½ÐµÑ‚ Ð½Ð¸ Ð¾Ð´Ð½Ð¾Ð³Ð¾ ÐºÐ»Ð°ÑÑÐ° Ð² Ð¿Ñ€Ð¾Ñ‚Ð¾Ñ‚Ð¸Ð¿Ð°Ñ…"

centroids = npz["centroids"].item()
stacks    = npz["stacks"].item()

centroid_t = {c: torch.tensor(v).float().to(device) for c,v in centroids.items()}
stacks_t   = {c: torch.tensor(v).float().to(device) for c,v in stacks.items()}

tfm_infer = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor()
])

def classify_crop(crop):
    x = tfm_infer(crop).unsqueeze(0).to(device)
    with torch.no_grad():
        z = clf_model(x)[0]     # [128], L2-Ð½Ð¾Ñ€Ð¼
    scores = {}
    for c in centroids.keys():
        s_centroid = F.cosine_similarity(z, centroid_t[c], dim=0)
        stk = stacks_t[c]
        s_knn = torch.topk(torch.mv(stk, z), k=min(TOPK, stk.shape[0]))[0].mean()
        scores[c] = ALPHA*s_centroid.item() + (1-ALPHA)*s_knn.item()
    ordered = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)
    (c1,s1),(c2,s2) = ordered[0], ordered[1] if len(ordered)>1 else (("none",-1))
    if s1 < THR_UNKNOWN or (s1 - s2) < THR_MARGIN:
        return "unknown", s1
    return c1, s1

def classwise_nms(dets, iou_thr=IOU_THR):
    # dets: class, score_metric, bbox_xyxy
    out=[]
    by_cls={}
    for d in dets:
        by_cls.setdefault(d["class"], []).append(d)
    for c, arr in by_cls.items():
        if len(arr)==1 or c=="unknown":
            out += arr
            continue
        boxes  = torch.tensor([d["bbox_xyxy"] for d in arr]).float()
        scores = torch.tensor([d["score_metric"] for d in arr]).float()
        keep = nms(boxes, scores, iou_thr).tolist()
        out += [arr[i] for i in keep]
    return out

with open(os.path.join(OUT_VLM,"predictions_vlm.json"),"r",encoding="utf-8") as f:
    vlm_preds = json.load(f)

merged=[]
for rec in vlm_preds:
    img_name = rec["image"]
    img_path = os.path.join(IMG_DIR, img_name)
    if not os.path.exists(img_path):
        print("skip missing", img_path); continue
    img = Image.open(img_path).convert("RGB")
    draw = ImageDraw.Draw(img)
    dets=[]
    for d in rec["detections"]:
        x1,y1,x2,y2 = d["bbox_xyxy"]
        # Ð»Ñ‘Ð³ÐºÐ¾Ðµ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð¸Ðµ Ð±Ð¾ÐºÑÐ° Ð½Ð° 10% Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð·Ð°Ñ…Ð²Ð°Ñ‚Ð¸Ñ‚ÑŒ ÐºÑ€Ð°Ñ
        w,h = x2-x1, y2-y1
        cx,cy = x1+w/2, y1+h/2
        x1n = max(0, cx - 0.55*w); y1n = max(0, cy - 0.55*h)
        x2n = min(img.width,  cx + 0.55*w); y2n = min(img.height, cy + 0.55*h)
        crop = img.crop((x1n,y1n,x2n,y2n)) # Ð²Ñ‹Ñ€ÐµÐ·Ð°ÐµÐ¼ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚

        label, conf = classify_crop(crop) # Ð¿Ñ€Ð¾Ð³Ð¾Ð½ÑÐµÐ¼ Ñ‡ÐµÑ€ÐµÐ· ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€
        dets.append({
            "bbox_xyxy":[float(x1n),float(y1n),float(x2n),float(y2n)],
            "class":label,
            "score_metric":float(conf),
            "score_vlm":float(d.get("score_vlm",0.0)),
            "vlm_class":d.get("class","")
        })

    dets = classwise_nms(dets, IOU_THR) # Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ nms

    # Ð²Ð¸Ð·ÑƒÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
    vis = img.copy(); draw = ImageDraw.Draw(vis)
    for d in dets:
        x1,y1,x2,y2 = d["bbox_xyxy"]
        color = (0,255,0) if d["class"]!="unknown" else (255,0,0)
        draw.rectangle([x1,y1,x2,y2], outline=color, width=3)
        draw.text((x1+3,y1+3), f'{d["class"]}:{d["score_metric"]:.2f}', fill=color)
    stem = os.path.splitext(img_name)[0]
    vis.save(os.path.join(OUT_METRIC, f"{stem}_final.jpg"))

    merged.append({"image": img_name, "detections": dets})

with open(os.path.join(OUT_METRIC,"predictions_final.json"),"w",encoding="utf-8") as f:
    json.dump(merged, f, ensure_ascii=False, indent=2)

print("âœ… Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸:", os.path.join(OUT_METRIC,"predictions_final.json"))


# âœ… Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸: /kaggle/working/out_metric/predictions_final.json