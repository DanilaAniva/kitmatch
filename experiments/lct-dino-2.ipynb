{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13114843,"sourceType":"datasetVersion","datasetId":8307759},{"sourceId":13115107,"sourceType":"datasetVersion","datasetId":8307936},{"sourceId":13119528,"sourceType":"datasetVersion","datasetId":8310910}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch, torchvision, torchaudio\nprint(\"torch:\", torch.__version__)\nprint(\"torchvision:\", torchvision.__version__)\nprint(\"torchaudio:\", torchaudio.__version__)\nprint(\"cuda:\", torch.version.cuda)\nimport numpy, scipy, sklearn\nprint(\"numpy:\", numpy.__version__)\nprint(\"scipy:\", scipy.__version__)\nprint(\"sklearn:\", sklearn.__version__)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:45:45.850068Z","iopub.execute_input":"2025-09-22T13:45:45.850384Z","iopub.status.idle":"2025-09-22T13:45:56.966901Z","shell.execute_reply.started":"2025-09-22T13:45:45.850360Z","shell.execute_reply":"2025-09-22T13:45:56.966263Z"}},"outputs":[{"name":"stdout","text":"torch: 2.6.0+cu124\ntorchvision: 0.21.0+cu124\ntorchaudio: 2.6.0+cu124\ncuda: 12.4\nnumpy: 1.26.4\nscipy: 1.15.3\nsklearn: 1.2.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, glob, json, random, math\nimport numpy as np\nimport contextlib\nfrom PIL import Image, ImageDraw\n\nBASE_DIR = \"/kaggle/working\"\n\nIMG_DIR      = \"/kaggle/input/lct-low/LCT/–ì—Ä—É–ø–ø–æ–≤—ã–µ –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\"  # –≥—Ä—É–ø–ø–æ–≤—ã–µ —Ñ–æ—Ç–æ\nCROPS_TRAIN  = \"/kaggle/input/lct-low/LCT\"                           # –∫—Ä–æ–ø—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ø–æ–¥–ø–∞–ø–∫–∏-–∫–ª–∞—Å—Å—ã)\nCROPS_REF    = \"/kaggle/input/lct-ref/REF\"                           # —ç—Ç–∞–ª–æ–Ω—ã (8‚Äì16 –Ω–∞ –∫–ª–∞—Å—Å)\n\nOUT_VLM      = os.path.join(BASE_DIR, \"out_vlm\")      # –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ—Ç GroundingDINO\nOUT_METRIC   = os.path.join(BASE_DIR, \"out_metric\")   # —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –≤–∏–∑—É–∞–ª–∫–∏ –∏ JSON\nBEST_PATH    = os.path.join(BASE_DIR, \"embedder_arcface_best.pth\")\nPROTO_PATH   = os.path.join(BASE_DIR, \"prototypes.npz\")\nPRED_VLM     = os.path.join(OUT_VLM, \"predictions_vlm.json\")\nPRED_FINAL   = os.path.join(OUT_METRIC, \"predictions_final.json\")     # —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –≤–∏–∑—É–∞–ª–∫–∏ –∏ JSON\n\nfor d in [IMG_DIR, CROPS_TRAIN, CROPS_REF, OUT_VLM, OUT_METRIC]:\n    os.makedirs(d, exist_ok=True)\n\n# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\nIMG_SIZE = 224\nBATCH    = 64\nEPOCHS   = 15\nLR       = 1e-3\nSEED     = 42\nNUM_WORKERS = 2\n# –ü–æ—Ä–æ–≥–æ–≤—ã–µ –ø–∞—Ä-—Ä—ã –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\nALPHA        = 0.5   # —Å–º–µ—à–∏–≤–∞–Ω–∏–µ centroid –∏ kNN\nTOPK         = 5\nTHR_UNKNOWN  = 0.10  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π cos –¥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞\nTHR_MARGIN   = 0.03  # –æ—Ç—Ä—ã–≤ top1-top2\nIOU_THR      = 0.55  # –¥–ª—è class-wise NMS\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:45:56.968107Z","iopub.execute_input":"2025-09-22T13:45:56.968478Z","iopub.status.idle":"2025-09-22T13:45:56.990531Z","shell.execute_reply.started":"2025-09-22T13:45:56.968459Z","shell.execute_reply":"2025-09-22T13:45:56.989793Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## –û–±—É—á–∏–º —ç–º–±–µ–¥–¥–µ—Ä: DinoV2 + ArcFace (–æ–±—É—á–µ–Ω–∏–µ)","metadata":{}},{"cell_type":"code","source":"# –ú–æ–¥–µ–ª—å\nMODEL_NAME = \"vit_base_patch14_dinov2.lvd142m\"  # –∏–ª–∏ \"vit_base_patch14_dinov2.lvd142m\"\nEMB_DIM    = 128\nFREEZE_BACKBONE = True      # 1-–π —ç—Ç–∞–ø: —É—á–∏–º –≥–æ–ª–æ–≤—É","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:45:56.991236Z","iopub.execute_input":"2025-09-22T13:45:56.991444Z","iopub.status.idle":"2025-09-22T13:45:56.995069Z","shell.execute_reply.started":"2025-09-22T13:45:56.991428Z","shell.execute_reply":"2025-09-22T13:45:56.994328Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ==== IMPORTS & DEVICE ====\nimport glob, json, random, math\nimport numpy as np\nfrom PIL import Image, ImageDraw\nimport unicodedata as ud\n\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport timm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"device:\", device)\n\ndef set_seed(seed=42):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\nset_seed(SEED)\n\ntry:\n    torch.set_float32_matmul_precision(\"high\")\nexcept Exception:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:45:56.996762Z","iopub.execute_input":"2025-09-22T13:45:56.996948Z","iopub.status.idle":"2025-09-22T13:46:01.167992Z","shell.execute_reply.started":"2025-09-22T13:45:56.996933Z","shell.execute_reply":"2025-09-22T13:46:01.167342Z"}},"outputs":[{"name":"stdout","text":"device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==== DATASET & UTILS ====\n\ndef _norm(s: str) -> str:\n    return ud.normalize(\"NFKC\", s).strip().lower()\n\ndef list_items(root):\n    assert os.path.isdir(root), f\"–ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {root}\"\n    SKIP_SUBSTR = [_norm(\"–≥—Ä—É–ø–ø–æ–≤—ã–µ\"), _norm(\"—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫\"), _norm(\"–ª–∏–Ω–µ–π–∫\")]\n\n    items, classes = [], []\n    for c in sorted(os.listdir(root)):\n        if c.startswith(\".\"): \n            continue\n        pth = os.path.join(root, c)\n        if not os.path.isdir(pth):\n            continue\n        if any(s in _norm(c) for s in SKIP_SUBSTR):\n            continue\n        classes.append(c)\n        for f in glob.glob(os.path.join(pth, \"*\")):\n            if f.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")):\n                items.append((f, c))\n\n    assert len(classes) > 0, \"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ CROPS_TRAIN\"\n    cls2id = {c:i for i,c in enumerate(classes)}\n    return items, cls2id\n\nclass ImgDS(Dataset):\n    def __init__(self, items, cls2id, tfm):\n        self.items, self.cls2id, self.tfm = items, cls2id, tfm\n    def __len__(self): return len(self.items)\n    def __getitem__(self, i):\n        p,c = self.items[i]\n        with Image.open(p).convert(\"RGB\") as im:\n            x = self.tfm(im)\n        return x, self.cls2id[c]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:46:01.168747Z","iopub.execute_input":"2025-09-22T13:46:01.168961Z","iopub.status.idle":"2025-09-22T13:46:01.177185Z","shell.execute_reply.started":"2025-09-22T13:46:01.168943Z","shell.execute_reply":"2025-09-22T13:46:01.176581Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ==== TRANSFORMS (timm) ====\n_tmp = timm.create_model(MODEL_NAME, pretrained=True, num_classes=0, global_pool=\"token\")\ndata_cfg = timm.data.resolve_model_data_config(_tmp)\nif IMG_SIZE is not None:\n    data_cfg[\"input_size\"] = (3, IMG_SIZE, IMG_SIZE)\n\ntfm_train = timm.data.create_transform(**data_cfg, is_training=True)\ntfm_val   = timm.data.create_transform(**data_cfg, is_training=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:46:01.178083Z","iopub.execute_input":"2025-09-22T13:46:01.178349Z","iopub.status.idle":"2025-09-22T13:46:04.180983Z","shell.execute_reply.started":"2025-09-22T13:46:01.178326Z","shell.execute_reply":"2025-09-22T13:46:04.180252Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f55a3e78c8445d88f3f27f1e482d52"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# ==== MODEL & ARC ====\n\nclass EmbedNet(nn.Module):\n    def __init__(self, dim=128, model_name=MODEL_NAME, pool=\"token\", freeze_backbone=False, img_size=IMG_SIZE):\n        super().__init__()\n        self.backbone = timm.create_model(\n            model_name,\n            pretrained=True,\n            num_classes=0,\n            global_pool=pool,\n            img_size=img_size  # —Å–æ–≥–ª–∞—Å—É–µ–º —Å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞–º–∏\n        )\n        # –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Ö–æ–¥ –Ω–µ —Ä–æ–≤–Ω–æ \"—Ä–æ–¥–Ω–æ–≥–æ\" —Ä–∞–∑–º–µ—Ä–∞ (–±–µ–∑ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π –ø–µ—Ä–µ—Å–µ–º–ø–ª–æ–≤–∫–∏ pos-—ç–º–±–µ–¥–æ–≤)\n        if hasattr(self.backbone, \"patch_embed\"):\n            try:\n                self.backbone.patch_embed.strict_img_size = False\n            except Exception:\n                pass\n        # –í–ê–ñ–ù–û: –ù–ï –≤–∫–ª—é—á–∞–µ–º dynamic_img_size ‚Äî —ç—Ç–æ –∏ –ª–æ–º–∞–ª–æ —Ñ–æ—Ä–º—É\n        # if hasattr(self.backbone, \"dynamic_img_size\"):\n        #     self.backbone.dynamic_img_size = True  # –£–î–ê–õ–ï–ù–û\n\n        feat_dim = self.backbone.num_features\n        if freeze_backbone:\n            for p in self.backbone.parameters():\n                p.requires_grad = False\n        self.head = nn.Sequential(\n            nn.Linear(feat_dim, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, dim),\n        )\n\n    def forward(self, x):\n        f = self.backbone(x)                 # [B, feat_dim]\n        z = F.normalize(self.head(f), dim=1) # [B, dim]\n        return z\n\nclass ArcMarginProduct(nn.Module):\n    def __init__(self, in_dim, n_classes, s=30.0, m=0.30):\n        super().__init__()\n        self.W = nn.Parameter(torch.randn(n_classes, in_dim))\n        nn.init.xavier_uniform_(self.W)\n        self.s, self.m = s, m\n    def forward(self, z, y):\n        W = F.normalize(self.W, dim=1)\n        cos = F.linear(z, W)\n        one_hot = F.one_hot(y, num_classes=W.size(0)).float().to(z.device)\n        return self.s * (cos - one_hot*self.m)\n\n# –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è\nmodel = EmbedNet(dim=EMB_DIM, freeze_backbone=FREEZE_BACKBONE, img_size=IMG_SIZE).to(device)\narc   = ArcMarginProduct(EMB_DIM, n_classes=1).to(device)  # –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏–º n_classes –Ω–∏–∂–µ\nce    = nn.CrossEntropyLoss()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:46:04.181865Z","iopub.execute_input":"2025-09-22T13:46:04.182067Z","iopub.status.idle":"2025-09-22T13:46:05.928926Z","shell.execute_reply.started":"2025-09-22T13:46:04.182051Z","shell.execute_reply":"2025-09-22T13:46:05.928386Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --- Albumentations –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —ç–º–±–µ–¥–¥–µ—Ä–∞ ---\n# pip: !pip -q install albumentations>=1.4.0  # (–Ω–∞ Kaggle –æ–±—ã—á–Ω–æ —É–∂–µ –µ—Å—Ç—å)\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# mean/std –±–µ—Ä—ë–º –∏–∑ timm data_cfg, —á—Ç–æ–±—ã –≤—Å—ë —Å–æ–≤–ø–∞–¥–∞–ª–æ —Å –±—ç–∫–±–æ–Ω–æ–º\n_MEAN = data_cfg.get('mean', (0.5, 0.5, 0.5))\n_STD  = data_cfg.get('std',  (0.5, 0.5, 0.5))\n\nIMG_PAD_VALUE = (114, 114, 114)  # —Ñ–æ–Ω —Å—Ç–æ–ª–∞/–±—É–º–∞–≥–∏\n\ntrain_aug = A.Compose([\n    A.LongestMaxSize(max_size=IMG_SIZE),\n    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=0, value=IMG_PAD_VALUE),\n    A.ShiftScaleRotate(shift_limit=0.04, scale_limit=0.15, rotate_limit=35,\n                       border_mode=0, value=IMG_PAD_VALUE, p=0.8),\n    A.HorizontalFlip(p=0.5),\n    A.Perspective(scale=(0.02, 0.06), p=0.2),\n    A.ColorJitter(0.3, 0.3, 0.3, 0.05, p=0.7),\n    A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n    A.CLAHE(clip_limit=(1, 3), p=0.1),\n    A.GaussNoise(var_limit=(5.0, 25.0), p=0.2),\n    A.MotionBlur(blur_limit=3, p=0.1),\n    A.ImageCompression(quality_lower=60, quality_upper=95, p=0.2),\n    A.CoarseDropout(max_holes=4,\n                    max_height=int(0.25*IMG_SIZE), max_width=int(0.25*IMG_SIZE),\n                    min_holes=1, min_height=16, min_width=16,\n                    fill_value=IMG_PAD_VALUE, p=0.5),\n    A.Normalize(mean=_MEAN, std=_STD),\n    ToTensorV2(),\n])\n\nval_aug = A.Compose([\n    A.LongestMaxSize(max_size=IMG_SIZE),\n    A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=0, value=IMG_PAD_VALUE),\n    A.Normalize(mean=_MEAN, std=_STD),\n    ToTensorV2(),\n])\n\n# --- Dataset, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç Albumentations ---\nclass AlbDS(torch.utils.data.Dataset):\n    def __init__(self, items, cls2id, aug):\n        self.items = items\n        self.cls2id = cls2id\n        self.aug = aug\n\n    def __len__(self): return len(self.items)\n\n    def __getitem__(self, i):\n        p, c = self.items[i]\n        # PIL -> numpy(BGR/RGB –Ω–µ–≤–∞–∂–Ω–æ –¥–ª—è A, –Ω–æ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º RGB)\n        img = np.array(Image.open(p).convert(\"RGB\"))\n        out = self.aug(image=img)\n        x = out[\"image\"]  # torch.Tensor [C,H,W], —É–∂–µ –Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω\n        y = self.cls2id[c]\n        return x, y\n\n# –º–∞–ª–µ–Ω—å–∫–∏–π –æ–±—ë—Ä—Ç—á–∏–∫, —á—Ç–æ–±—ã —Ç–µ –∂–µ —Å–∞–º—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å\n# –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤ –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∫—Ä–æ–ø–æ–≤: –≤—ã–∑–æ–≤ –∫–∞–∫ tfm(image_PIL) -> torch.Tensor\nclass AlbWrap:\n    def __init__(self, aug): self.aug = aug\n    def __call__(self, pil_img):\n        arr = np.array(pil_img.convert(\"RGB\"))\n        return self.aug(image=arr)[\"image\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:46:05.929726Z","iopub.execute_input":"2025-09-22T13:46:05.929938Z","iopub.status.idle":"2025-09-22T13:46:06.621922Z","shell.execute_reply.started":"2025-09-22T13:46:05.929920Z","shell.execute_reply":"2025-09-22T13:46:06.621183Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3459448938.py:14: UserWarning: Argument(s) 'value' are not valid for transform PadIfNeeded\n  A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=0, value=IMG_PAD_VALUE),\n/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n/tmp/ipykernel_36/3459448938.py:15: UserWarning: Argument(s) 'value' are not valid for transform ShiftScaleRotate\n  A.ShiftScaleRotate(shift_limit=0.04, scale_limit=0.15, rotate_limit=35,\n/tmp/ipykernel_36/3459448938.py:22: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n  A.GaussNoise(var_limit=(5.0, 25.0), p=0.2),\n/tmp/ipykernel_36/3459448938.py:24: UserWarning: Argument(s) 'quality_lower, quality_upper' are not valid for transform ImageCompression\n  A.ImageCompression(quality_lower=60, quality_upper=95, p=0.2),\n/tmp/ipykernel_36/3459448938.py:25: UserWarning: Argument(s) 'max_holes, max_height, max_width, min_holes, min_height, min_width, fill_value' are not valid for transform CoarseDropout\n  A.CoarseDropout(max_holes=4,\n/tmp/ipykernel_36/3459448938.py:35: UserWarning: Argument(s) 'value' are not valid for transform PadIfNeeded\n  A.PadIfNeeded(IMG_SIZE, IMG_SIZE, border_mode=0, value=IMG_PAD_VALUE),\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ==== DATA & TRAIN ====\nitems, cls2id = list_items(CROPS_TRAIN)\ntrain_items, val_items = train_test_split(items, test_size=0.2,\n                                          stratify=[c for _,c in items], random_state=SEED)\n\n# –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º arc —Å —Ä–µ–∞–ª—å–Ω—ã–º —á–∏—Å–ª–æ–º –∫–ª–∞—Å—Å–æ–≤\ndel arc\narc = ArcMarginProduct(EMB_DIM, n_classes=len(cls2id)).to(device)\n\npin = torch.cuda.is_available()\ndl_train = DataLoader(AlbDS(train_items, cls2id, train_aug), batch_size=BATCH, shuffle=True,\n                      num_workers=NUM_WORKERS, pin_memory=pin, persistent_workers=pin and NUM_WORKERS>0)\ndl_val   = DataLoader(AlbDS(val_items,   cls2id, val_aug),   batch_size=BATCH, shuffle=False,\n                      num_workers=NUM_WORKERS, pin_memory=pin, persistent_workers=pin and NUM_WORKERS>0)\n\n# –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\nif FREEZE_BACKBONE:\n    params = list(model.head.parameters()) + list(arc.parameters())\n    opt = torch.optim.AdamW(params, lr=LR, weight_decay=1e-4)\nelse:\n    opt = torch.optim.AdamW([\n        {\"params\": model.backbone.parameters(), \"lr\": max(LR*0.2, 5e-5), \"weight_decay\": 0.05},\n        {\"params\": model.head.parameters(),     \"lr\": LR,                \"weight_decay\": 1e-4},\n        {\"params\": arc.parameters(),            \"lr\": LR,                \"weight_decay\": 1e-4},\n    ])\n\nscaler = torch.amp.GradScaler(device='cuda', enabled=torch.cuda.is_available())\nautocast_ctx = (torch.amp.autocast(device_type='cuda', dtype=torch.float16, enabled=torch.cuda.is_available())\n                if torch.cuda.is_available()\n                else contextlib.nullcontext())\nbest_acc = 0.0\nos.makedirs(os.path.dirname(BEST_PATH), exist_ok=True)\n\nfor ep in range(1, EPOCHS+1):\n    model.train(); arc.train()\n    tot=0; cnt=0\n    pbar = tqdm(dl_train, desc=f\"Epoch {ep}/{EPOCHS}\", leave=False)\n    for x,y in pbar:\n        x,y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n        with autocast_ctx:\n            z = model(x)\n            logits = arc(z, y)\n            loss = ce(logits, y)\n        opt.zero_grad(set_to_none=True)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        tot += loss.item()*x.size(0); cnt += x.size(0)\n        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n    model.eval(); arc.eval()\n    correct=0; total=0\n    with torch.no_grad():\n        for x,y in dl_val:\n            x,y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n                z = model(x); logits = arc(z,y)\n            pred = logits.argmax(1)\n            correct += (pred==y).sum().item(); total += y.size(0)\n\n    val_acc = correct / max(1,total)\n    print(f\"Epoch {ep:02d} | train_loss={tot/max(1,cnt):.4f} | val_acc={val_acc:.3f}\")\n\n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save({\n            \"model\": model.state_dict(),\n            \"cls2id\": cls2id,\n            \"epoch\": ep,\n            \"val_acc\": val_acc,\n            \"model_name\": MODEL_NAME,\n            \"emb_dim\": EMB_DIM,\n            \"freeze_backbone\": FREEZE_BACKBONE,\n            \"data_cfg\": data_cfg,        # <-- —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–æ–≤!\n            \"img_size\": IMG_SIZE         # <-- –∏ —Ä–∞–∑–º–µ—Ä\n        }, BEST_PATH)\n        print(f\"üåü –ù–æ–≤—ã–π –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {BEST_PATH} (acc={val_acc:.3f})\")\n\nprint(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –õ—É—á—à–∏–π acc =\", best_acc)\n\n# —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º—ã –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞/–ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤\ntfm_proto = AlbWrap(val_aug)   # —Ç–µ –∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è/–ø–∞–¥–¥–∏–Ω–≥/resize\ntfm_infer = tfm_proto\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T13:46:06.622778Z","iopub.execute_input":"2025-09-22T13:46:06.623015Z","iopub.status.idle":"2025-09-22T14:05:23.117827Z","shell.execute_reply.started":"2025-09-22T13:46:06.622988Z","shell.execute_reply":"2025-09-22T14:05:23.116796Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/2822426495.py:56: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train_loss=9.1925 | val_acc=0.536\nüåü –ù–æ–≤—ã–π –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: /kaggle/working/embedder_arcface_best.pth (acc=0.536)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 02 | train_loss=5.1158 | val_acc=0.573\nüåü –ù–æ–≤—ã–π –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: /kaggle/working/embedder_arcface_best.pth (acc=0.573)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 03 | train_loss=4.3329 | val_acc=0.691\nüåü –ù–æ–≤—ã–π –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: /kaggle/working/embedder_arcface_best.pth (acc=0.691)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 04 | train_loss=3.6130 | val_acc=0.800\nüåü –ù–æ–≤—ã–π –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: /kaggle/working/embedder_arcface_best.pth (acc=0.800)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 05 | train_loss=3.6619 | val_acc=0.782\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 06 | train_loss=3.1171 | val_acc=0.782\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 07 | train_loss=3.1681 | val_acc=0.827\nüåü –ù–æ–≤—ã–π –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: /kaggle/working/embedder_arcface_best.pth (acc=0.827)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 08 | train_loss=2.6209 | val_acc=0.809\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 09 | train_loss=2.7231 | val_acc=0.809\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10 | train_loss=2.5369 | val_acc=0.818\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11 | train_loss=2.4923 | val_acc=0.827\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12 | train_loss=2.4503 | val_acc=0.827\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13 | train_loss=2.0454 | val_acc=0.864\nüåü –ù–æ–≤—ã–π –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: /kaggle/working/embedder_arcface_best.pth (acc=0.864)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14 | train_loss=2.2852 | val_acc=0.873\nüåü –ù–æ–≤—ã–π –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: /kaggle/working/embedder_arcface_best.pth (acc=0.873)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15/15:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15 | train_loss=2.0628 | val_acc=0.873\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –õ—É—á—à–∏–π acc = 0.8727272727272727\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os, torch\nckpt_path = \"/kaggle/working/embedder_arcface_best.pth\"\nprint(\"–§–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç:\", os.path.exists(ckpt_path), \"| —Ä–∞–∑–º–µ—Ä, –±–∞–π—Ç:\", os.path.getsize(ckpt_path) if os.path.exists(ckpt_path) else 0)\n\nif os.path.exists(ckpt_path):\n    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n    print(\"–ö–ª—é—á–∏ –≤ —á–µ–∫–ø–æ–π–Ω—Ç–µ:\", list(ckpt.keys()))\n    print(\"–ß–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤:\", len(ckpt[\"cls2id\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:05:23.121454Z","iopub.execute_input":"2025-09-22T14:05:23.121758Z","iopub.status.idle":"2025-09-22T14:05:23.405316Z","shell.execute_reply.started":"2025-09-22T14:05:23.121720Z","shell.execute_reply":"2025-09-22T14:05:23.404525Z"}},"outputs":[{"name":"stdout","text":"–§–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: True | —Ä–∞–∑–º–µ—Ä, –±–∞–π—Ç: 344820798\n–ö–ª—é—á–∏ –≤ —á–µ–∫–ø–æ–π–Ω—Ç–µ: ['model', 'cls2id', 'epoch', 'val_acc', 'model_name', 'emb_dim', 'freeze_backbone', 'data_cfg', 'img_size']\n–ß–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤: 11\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"for cls, idx in ckpt[\"cls2id\"].items():\n    print(f\"{idx:2d} ‚Üí {cls}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:05:23.406288Z","iopub.execute_input":"2025-09-22T14:05:23.406817Z","iopub.status.idle":"2025-09-22T14:05:23.410768Z","shell.execute_reply.started":"2025-09-22T14:05:23.406795Z","shell.execute_reply":"2025-09-22T14:05:23.410093Z"}},"outputs":[{"name":"stdout","text":" 0 ‚Üí 1 –û—Ç–≤–µ—Ä—Ç–∫–∞ ¬´-¬ª\n 1 ‚Üí 10 –ö–ª—é—á —Ä–æ–∂–∫–æ–≤—ã–∏ÃÜ–Ω–∞–∫–∏–¥–Ω–æ–∏ÃÜ  ¬æ\n 2 ‚Üí 11 –ë–æ–∫–æ—Ä–µ–∑—ã\n 3 ‚Üí 2 –û—Ç–≤–µ—Ä—Ç–∫–∞ ¬´+¬ª\n 4 ‚Üí 3 –û—Ç–≤–µ—Ä—Ç–∫–∞ –Ω–∞ —Å–º–µ—â–µ–Ω–Ω—ã–∏ÃÜ –∫—Ä–µ—Å—Ç\n 5 ‚Üí 4 –ö–æ–ª–æ–≤–æ—Ä–æ—Ç\n 6 ‚Üí 5 –ü–∞—Å—Å–∞—Ç–∏–∂–∏ –∫–æ–Ω—Ç—Ä–æ–≤–æ—á–Ω—ã–µ\n 7 ‚Üí 6 –ü–∞—Å—Å–∞—Ç–∏–∂–∏\n 8 ‚Üí 7 –®—ç—Ä–Ω–∏—Ü–∞\n 9 ‚Üí 8 –†–∞–∑–≤–æ–¥–Ω–æ–∏ÃÜ –∫–ª—é—á\n10 ‚Üí 9 –û—Ç–∫—Ä—ã–≤–∞—à–∫–∞ –¥–ª—è –±–∞–Ω–æ–∫ —Å –º–∞—Å–ª–æ–º\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## –ü–æ–ª—É—á–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –∫–ª–∞—Å—Å–æ–≤ (—Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã + —Å—Ç–µ–∫–∏ —ç—Ç–∞–ª–æ–Ω–æ–≤)","metadata":{}},{"cell_type":"code","source":"# ==== PROTOTYPES ====\nimport numpy as np\n\nckpt = torch.load(BEST_PATH, map_location=\"cpu\")\ncls2id = ckpt[\"cls2id\"]; id2cls = {v:k for k,v in cls2id.items()}\n\n# —Å–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å –Ω–∞ CPU —Å —Ç–µ–º –∂–µ img_size, —á—Ç–æ –≤ ckpt\nproto_model = EmbedNet(EMB_DIM, model_name=ckpt.get(\"model_name\", MODEL_NAME),\n                       freeze_backbone=True, img_size=ckpt.get(\"img_size\", IMG_SIZE))\nproto_model.load_state_dict(ckpt[\"model\"])\nproto_model.eval()\n\ncentroids, stacks = {}, {}\nwith torch.no_grad():\n    for c in sorted(os.listdir(CROPS_REF)):\n        pth = os.path.join(CROPS_REF, c)\n        if not os.path.isdir(pth): \n            continue\n        vecs=[]\n        for f in glob.glob(os.path.join(pth, \"*\")):\n            if not f.lower().endswith((\".jpg\",\".png\",\".jpeg\",\".bmp\",\".webp\")):\n                continue\n            with Image.open(f).convert(\"RGB\") as im:\n                x = tfm_proto(im).unsqueeze(0)  # CPU\n            z = proto_model(x).squeeze(0).numpy()\n            z /= max(np.linalg.norm(z), 1e-12)\n            vecs.append(z)\n        if vecs:\n            M = np.stack(vecs, axis=0).astype(np.float32)\n            mu = M.mean(0); mu /= max(np.linalg.norm(mu), 1e-12)\n            centroids[c] = mu\n            stacks[c]    = M\n\nnp.savez_compressed(PROTO_PATH,\n                    centroids=centroids, stacks=stacks, id2cls=id2cls, cls2id=cls2id)\nprint(\"‚úÖ –°–æ—Ö—Ä–∞–Ω–∏–ª–∏\", PROTO_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:05:23.411609Z","iopub.execute_input":"2025-09-22T14:05:23.412154Z","iopub.status.idle":"2025-09-22T14:06:37.464298Z","shell.execute_reply.started":"2025-09-22T14:05:23.412114Z","shell.execute_reply":"2025-09-22T14:06:37.463493Z"}},"outputs":[{"name":"stdout","text":"‚úÖ –°–æ—Ö—Ä–∞–Ω–∏–ª–∏ /kaggle/working/prototypes.npz\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"NPZ = \"/kaggle/working/prototypes.npz\"\nnpz = np.load(NPZ, allow_pickle=True)\ncentroids = npz[\"centroids\"].item()\nstacks    = npz[\"stacks\"].item()\nprint(\"\\n–ö–ª–∞—Å—Å–æ–≤ –≤ prototypes.npz:\", len(centroids))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:06:37.465299Z","iopub.execute_input":"2025-09-22T14:06:37.465602Z","iopub.status.idle":"2025-09-22T14:06:37.474307Z","shell.execute_reply.started":"2025-09-22T14:06:37.465582Z","shell.execute_reply":"2025-09-22T14:06:37.473478Z"}},"outputs":[{"name":"stdout","text":"\n–ö–ª–∞—Å—Å–æ–≤ –≤ prototypes.npz: 11\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## GroundingDino: –¥–µ—Ç–µ–∫—Ü–∏—è –±–æ–∫—Å–æ–≤ –ø–æ –ø—Ä–æ–º–ø—Ç–∞–º (–±–µ–∑ –æ–±—É—á–µ–Ω–∏—è)","metadata":{}},{"cell_type":"code","source":"#pip install -q \"transformers==4.44.2\" \"accelerate>=0.21\" safetensors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:06:37.475169Z","iopub.execute_input":"2025-09-22T14:06:37.475464Z","iopub.status.idle":"2025-09-22T14:06:37.491837Z","shell.execute_reply.started":"2025-09-22T14:06:37.475439Z","shell.execute_reply":"2025-09-22T14:06:37.491023Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ==== GroundingDINO: –¥–µ—Ç–µ–∫—Ü–∏—è –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º ====\nimport transformers as hf\nfrom transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\nfrom torchvision.ops import nms\n\nprint(\"transformers:\", hf.__version__)\nprint(\"torch:\", torch.__version__)\nuse_cuda = torch.cuda.is_available()\ngd_device = \"cuda\" if use_cuda else \"cpu\"\n\n# –ö–ª–∞—Å—Å-–ª–∏—Å—Ç\nclass_list = [\n    \"tool\"\n]\nidx2class  = {i:c for i,c in enumerate(class_list)}\ntext_prompt = \" . \".join(class_list) + \" .\"\n\nMODEL_NAME_DINO = \"IDEA-Research/grounding-dino-base\"  # tiny –±—ã—Å—Ç—Ä–µ–µ; base —Ç–æ—á–Ω–µ–µ\nprocessor  = AutoProcessor.from_pretrained(MODEL_NAME_DINO)\ngdinomodel = AutoModelForZeroShotObjectDetection.from_pretrained(MODEL_NAME_DINO).to(gd_device).eval()\n\nBOX_THR, TEXT_THR = 0.25, 0.25\nVLM_IOU_THR, TOPK_PER_CLS = 0.55, 50\nDO_VIS, BATCH_SIZE, SAVE_EVERY, RESUME = False, 2, 50, False\n\nall_imgs = sorted([p for p in glob.glob(os.path.join(IMG_DIR, \"*\"))\n                   if p.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\"))])\nprint(f\"üñºÔ∏è –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(all_imgs)}\")\n\nvlm_results = []\ndone_set = set()\npred_path = PRED_VLM  # <-- –ë–´–õ–û –ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–û, —Ç–µ–ø–µ—Ä—å –µ—Å—Ç—å\n\ndef save_json_atomic(path, data):\n    tmp = path + \".tmp\"\n    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n        json.dump(data, f, ensure_ascii=False, indent=2)\n    os.replace(tmp, path)\n\ndef to_tensor_safe(x, dtype=None, dev=\"cpu\"):\n    if isinstance(x, torch.Tensor):\n        return x.to(device=dev, dtype=dtype) if (dtype or x.device.type != dev) else x\n    return torch.tensor(x, dtype=dtype, device=dev)\n\ndef extract_label_names(res, idx2class):\n    for k in (\"text_labels\",\"texts\",\"phrases\"):\n        if k in res and res[k] is not None:\n            try:\n                return [str(s) for s in (res[k].tolist() if isinstance(res[k], torch.Tensor) else list(res[k]))]\n            except Exception:\n                pass\n    for k in (\"labels\",\"label_ids\",\"label_indices\"):\n        if k in res and res[k] is not None:\n            raw = res[k]\n            if isinstance(raw, torch.Tensor):\n                idxs = raw.detach().cpu().to(torch.int64).tolist()\n            else:\n                seq = list(raw)\n                if not seq: return []\n                if isinstance(seq[0], str): return [str(s) for s in seq]\n                try: idxs = [int(i) for i in seq]\n                except Exception: return [str(x) for x in seq]\n            return [idx2class[i] if 0 <= int(i) < len(idx2class) else None for i in idxs]\n    return []\n\ndef run_batch(paths):\n    imgs = [Image.open(p).convert(\"RGB\") for p in paths]\n    inputs = processor(images=imgs, text=[text_prompt]*len(imgs), return_tensors=\"pt\").to(gd_device)\n    with torch.no_grad():\n        if use_cuda:\n            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n                outputs = gdinomodel(**inputs)\n        else:\n            outputs = gdinomodel(**inputs)\n\n    results = processor.post_process_grounded_object_detection(\n        outputs=outputs, input_ids=inputs.input_ids,\n        box_threshold=BOX_THR, text_threshold=TEXT_THR,\n        target_sizes=[im.size[::-1] for im in imgs],\n    )\n\n    batch_dets=[]\n    for im,res in zip(imgs, results):\n        boxes  = to_tensor_safe(res.get(\"boxes\", []),  dtype=torch.float32, dev=\"cpu\")\n        scores = to_tensor_safe(res.get(\"scores\", []), dtype=torch.float32, dev=\"cpu\")\n        labels_iter = extract_label_names(res, idx2class)\n\n        n = min(len(boxes), len(scores), len(labels_iter))\n        if n == 0:\n            batch_dets.append([])\n            continue\n        boxes, scores, labels_iter = boxes[:n], scores[:n], labels_iter[:n]\n\n        dets_by_cls = {}\n        for bb, sc, name in zip(boxes, scores, labels_iter):\n            if not name: continue\n            dets_by_cls.setdefault(name, []).append((bb, float(sc)))\n\n        dets = []\n        for cls_name, arr in dets_by_cls.items():\n            b = torch.stack([a[0] if isinstance(a[0], torch.Tensor) else torch.tensor(a[0], dtype=torch.float32) for a in arr], dim=0)\n            s = torch.tensor([a[1] for a in arr], dtype=torch.float32)\n            if use_cuda: b,s = b.to(\"cuda\"), s.to(\"cuda\")\n            if len(s) > TOPK_PER_CLS:\n                topk = torch.topk(s, TOPK_PER_CLS).indices\n                b, s = b[topk], s[topk]\n            if b.numel()==0: continue\n            keep = nms(b, s, VLM_IOU_THR)\n            b, s = b[keep].to(\"cpu\"), s[keep].to(\"cpu\")\n            for bb, sc in zip(b, s):\n                x1,y1,x2,y2 = [float(v) for v in bb.tolist()]\n                dets.append({\"class\": cls_name, \"score_vlm\": float(sc), \"bbox_xyxy\": [x1,y1,x2,y2]})\n        batch_dets.append(dets)\n\n    for im in imgs:\n        try: im.close()\n        except Exception: pass\n\n    return batch_dets\n\nbuf=[]\nfor i in tqdm(range(0, len(all_imgs), BATCH_SIZE), desc=\"GroundingDINO\"):\n    chunk = all_imgs[i:i+BATCH_SIZE]\n    try:\n        dets_list = run_batch(chunk)\n    except Exception as e:\n        print(f\"‚ö†Ô∏è –æ—à–∏–±–∫–∞ –≤ run_batch [{i}:{i+BATCH_SIZE}]: {e}\")\n        continue\n\n    if DO_VIS:\n        for p, dets in zip(chunk, dets_list):\n            try:\n                with Image.open(p).convert(\"RGB\") as im:\n                    d = ImageDraw.Draw(im)\n                    for det in dets:\n                        x1,y1,x2,y2 = det[\"bbox_xyxy\"]\n                        d.rectangle([x1,y1,x2,y2], outline=(0,255,0), width=3)\n                        d.text((max(0,x1+3), max(0,y1-14)), f'{det[\"class\"]}:{det[\"score_vlm\"]:.2f}', fill=(0,255,0))\n                    im.save(os.path.join(OUT_VLM, os.path.splitext(os.path.basename(p))[0] + \"_vlm.jpg\"))\n            except Exception as e:\n                print(\"‚ö†Ô∏è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è:\", e)\n\n    for p, dets in zip(chunk, dets_list):\n        rec = {\"image\": os.path.basename(p), \"detections\": dets}\n        vlm_results.append(rec); buf.append(rec)\n\n    if len(buf) >= SAVE_EVERY:\n        save_json_atomic(pred_path, vlm_results)\n        buf.clear()\n\nif buf or not os.path.exists(pred_path):\n    save_json_atomic(pred_path, vlm_results)\n\nprint(\"‚úÖ GroundingDINO –≥–æ—Ç–æ–≤–æ:\", pred_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:06:37.492859Z","iopub.execute_input":"2025-09-22T14:06:37.493180Z","iopub.status.idle":"2025-09-22T14:16:01.730373Z","shell.execute_reply.started":"2025-09-22T14:06:37.493129Z","shell.execute_reply":"2025-09-22T14:16:01.729423Z"}},"outputs":[{"name":"stderr","text":"2025-09-22 14:06:40.882811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758550001.086182      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758550001.143634      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"transformers: 4.52.4\ntorch: 2.6.0+cu124\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/457 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ccbf3a5b47e4df696f049c9c7b1bd88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8af338378c04108adeef050c7872ec6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f53e69a882d4852a959d12253f6d964"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b27f8e7a53bb43d5a2038e17b8eada20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d599c0fbe8a54ab1ba9cc66993f036af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c4cbb901d214deb84b38f94f1b6d21b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/933M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7ef6f4a39af44bab56c27d4f4477b72"}},"metadata":{}},{"name":"stdout","text":"üñºÔ∏è –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 510\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"GroundingDINO:   0%|          | 0/255 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9143edb2fbd41e7bbfbebd63b06f0fa"}},"metadata":{}},{"name":"stdout","text":"‚úÖ GroundingDINO –≥–æ—Ç–æ–≤–æ: /kaggle/working/out_vlm/predictions_vlm.json\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## –ú–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –ø–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è + class-wise NMS","metadata":{}},{"cell_type":"code","source":"from torchvision.ops import nms\nfrom torchvision import transforms\n# –∑–∞–≥—Ä—É–∑–∫–∞ —ç–º–±–µ–¥–¥–µ—Ä–∞ –∏ –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤\nckpt = torch.load(os.path.join(BASE_DIR,\"embedder_arcface_best.pth\"), map_location=device)\nclf_model = EmbedNet(128).to(device)\nclf_model.load_state_dict(ckpt[\"model\"])\nclf_model.eval()\n\nnpz = np.load(os.path.join(BASE_DIR,\"prototypes.npz\"), allow_pickle=True)\n# –∞—Å—Å–µ—Ä—Ç –¥–ª—è –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–≤\nassert len(centroids) > 0, \"centroids –ø—É—Å—Ç–æ–π ‚Äî –ø–µ—Ä–µ—Å–æ–∑–¥–∞–π prototypes.npz –∏–∑ CROPS_REF\"\nfor c, M in stacks.items():\n    if M is None or len(M) == 0:\n        print(f\"‚ö†Ô∏è –í –∫–ª–∞—Å—Å–µ '{c}' –Ω–µ—Ç —ç—Ç–∞–ª–æ–Ω–æ–≤ –≤ stacks ‚Äî –ø—Ä–æ–ø—É—â—É –µ–≥–æ\")\n# –æ—Ç–±—Ä–æ—Å–∏–º –∫–ª–∞—Å—Å—ã –±–µ–∑ —Å—Ç–µ–∫–æ–≤\ncentroids = {c:v for c,v in centroids.items() if c in stacks and len(stacks[c])>0}\nstacks    = {c:stacks[c] for c in centroids.keys()}\nassert len(centroids) > 0, \"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞—Ö\"\n\ncentroids = npz[\"centroids\"].item()\nstacks    = npz[\"stacks\"].item()\n\ncentroid_t = {c: torch.tensor(v).float().to(device) for c,v in centroids.items()}\nstacks_t   = {c: torch.tensor(v).float().to(device) for c,v in stacks.items()}\n\ntfm_infer = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor()\n])\n\ndef classify_crop(crop):\n    x = tfm_infer(crop).unsqueeze(0).to(device)\n    with torch.no_grad():\n        z = clf_model(x)[0]     # [128], L2-–Ω–æ—Ä–º\n    scores = {}\n    for c in centroids.keys():\n        s_centroid = F.cosine_similarity(z, centroid_t[c], dim=0)\n        stk = stacks_t[c]\n        s_knn = torch.topk(torch.mv(stk, z), k=min(TOPK, stk.shape[0]))[0].mean()\n        scores[c] = ALPHA*s_centroid.item() + (1-ALPHA)*s_knn.item()\n    ordered = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)\n    (c1,s1),(c2,s2) = ordered[0], ordered[1] if len(ordered)>1 else ((\"none\",-1))\n    if s1 < THR_UNKNOWN or (s1 - s2) < THR_MARGIN:\n        return \"unknown\", s1\n    return c1, s1\n\ndef classwise_nms(dets, iou_thr=IOU_THR):\n    # dets: class, score_metric, bbox_xyxy\n    out=[]\n    by_cls={}\n    for d in dets:\n        by_cls.setdefault(d[\"class\"], []).append(d)\n    for c, arr in by_cls.items():\n        if len(arr)==1 or c==\"unknown\":\n            out += arr\n            continue\n        boxes  = torch.tensor([d[\"bbox_xyxy\"] for d in arr]).float()\n        scores = torch.tensor([d[\"score_metric\"] for d in arr]).float()\n        keep = nms(boxes, scores, iou_thr).tolist()\n        out += [arr[i] for i in keep]\n    return out\n\nwith open(os.path.join(OUT_VLM,\"predictions_vlm.json\"),\"r\",encoding=\"utf-8\") as f:\n    vlm_preds = json.load(f)\n\nmerged=[]\nfor rec in vlm_preds:\n    img_name = rec[\"image\"]\n    img_path = os.path.join(IMG_DIR, img_name)\n    if not os.path.exists(img_path):\n        print(\"skip missing\", img_path); continue\n    img = Image.open(img_path).convert(\"RGB\")\n    draw = ImageDraw.Draw(img)\n    dets=[]\n    for d in rec[\"detections\"]:\n        x1,y1,x2,y2 = d[\"bbox_xyxy\"]\n        # –ª—ë–≥–∫–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –±–æ–∫—Å–∞ –Ω–∞ 10% —á—Ç–æ–±—ã –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –∫—Ä–∞—è\n        w,h = x2-x1, y2-y1\n        cx,cy = x1+w/2, y1+h/2\n        x1n = max(0, cx - 0.55*w); y1n = max(0, cy - 0.55*h)\n        x2n = min(img.width,  cx + 0.55*w); y2n = min(img.height, cy + 0.55*h)\n        crop = img.crop((x1n,y1n,x2n,y2n)) # –≤—ã—Ä–µ–∑–∞–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\n\n        label, conf = classify_crop(crop) # –ø—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä\n        dets.append({\n            \"bbox_xyxy\":[float(x1n),float(y1n),float(x2n),float(y2n)],\n            \"class\":label,\n            \"score_metric\":float(conf),\n            \"score_vlm\":float(d.get(\"score_vlm\",0.0)),\n            \"vlm_class\":d.get(\"class\",\"\")\n        })\n\n    dets = classwise_nms(dets, IOU_THR) # –ø—Ä–∏–º–µ–Ω—è–µ–º nms\n\n    # –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n    vis = img.copy(); draw = ImageDraw.Draw(vis)\n    for d in dets:\n        x1,y1,x2,y2 = d[\"bbox_xyxy\"]\n        color = (0,255,0) if d[\"class\"]!=\"unknown\" else (255,0,0)\n        draw.rectangle([x1,y1,x2,y2], outline=color, width=3)\n        draw.text((x1+3,y1+3), f'{d[\"class\"]}:{d[\"score_metric\"]:.2f}', fill=color)\n    stem = os.path.splitext(img_name)[0]\n    vis.save(os.path.join(OUT_METRIC, f\"{stem}_final.jpg\"))\n\n    merged.append({\"image\": img_name, \"detections\": dets})\n\nwith open(os.path.join(OUT_METRIC,\"predictions_final.json\"),\"w\",encoding=\"utf-8\") as f:\n    json.dump(merged, f, ensure_ascii=False, indent=2)\n\nprint(\"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–µ—Ç–µ–∫—Ü–∏–∏:\", os.path.join(OUT_METRIC,\"predictions_final.json\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-22T14:16:01.731527Z","iopub.execute_input":"2025-09-22T14:16:01.732541Z","iopub.status.idle":"2025-09-22T14:22:08.101844Z","shell.execute_reply.started":"2025-09-22T14:16:01.732518Z","shell.execute_reply":"2025-09-22T14:22:08.100833Z"}},"outputs":[{"name":"stdout","text":"‚úÖ –§–∏–Ω–∞–ª—å–Ω—ã–µ –¥–µ—Ç–µ–∫—Ü–∏–∏: /kaggle/working/out_metric/predictions_final.json\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}